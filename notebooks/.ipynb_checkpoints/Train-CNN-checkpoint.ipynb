{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# this is for loading the data\n",
    "def load_fall_X(file_name):\n",
    "    temp = np.memmap(file_name, dtype='float32', mode='r')\n",
    "    X = np.reshape(temp, [-1, 256, 6])\n",
    "    return X\n",
    "\n",
    "# this is for loading the labels (one-hot encoding: [1, 0, 0]-->nonfall, [0, 1, 0]-->pre-impact fall, [0, 0, 1]-->fall\t\n",
    "def load_fall_y(file_name):\n",
    "    temp = np.memmap(file_name, dtype='int8', mode='r')\n",
    "    y= np.reshape(temp, [-1, 3])\n",
    "    return y\n",
    "\n",
    "def dset_fpath(fname):\n",
    "    _fp = os.path.abspath('.')\n",
    "    _hp = \"wobblewatch\"\n",
    "    _fp = _fp.split(_hp)[0]\n",
    "    return os.path.abspath(\"%s/%s/data/pre-processed_dataset/%s\" % (_fp, _hp, fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test  = {'X': {}, 'y': {}}\n",
    "train = {'X': {}, 'y': {}}\n",
    "for i in range(5):\n",
    "    test['X'][i] = load_fall_X(dset_fpath(\"test_x_%i\" % i))\n",
    "    test['y'][i] = load_fall_y(dset_fpath(\"test_y_%i\" % i))\n",
    "    train['X'][i] = load_fall_X(dset_fpath(\"train_x_%i\" % i))\n",
    "    train['y'][i] = load_fall_y(dset_fpath(\"train_y_%i\" % i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 0\n",
      " X train shape: (75025, 256, 6)\n",
      " y train shape: (75025, 3)\n",
      " X test shape:  (19064, 256, 6)\n",
      " y test shape:  (19064, 3)\n",
      "Set 1\n",
      " X train shape: (73347, 256, 6)\n",
      " y train shape: (73347, 3)\n",
      " X test shape:  (20742, 256, 6)\n",
      " y test shape:  (20742, 3)\n",
      "Set 2\n",
      " X train shape: (73534, 256, 6)\n",
      " y train shape: (73534, 3)\n",
      " X test shape:  (20555, 256, 6)\n",
      " y test shape:  (20555, 3)\n",
      "Set 3\n",
      " X train shape: (77296, 256, 6)\n",
      " y train shape: (77296, 3)\n",
      " X test shape:  (16793, 256, 6)\n",
      " y test shape:  (16793, 3)\n",
      "Set 4\n",
      " X train shape: (77154, 256, 6)\n",
      " y train shape: (77154, 3)\n",
      " X test shape:  (16935, 256, 6)\n",
      " y test shape:  (16935, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Set\", i)\n",
    "    print(\" X train shape:\", train['X'][i].shape)\n",
    "    print(\" y train shape:\", train['y'][i].shape)\n",
    "    print(\" X test shape: \", test['X'][i].shape)\n",
    "    print(\" y test shape: \", test['y'][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: memmap([[1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0]], dtype=int8),\n",
       " 1: memmap([[1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0]], dtype=int8),\n",
       " 2: memmap([[1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0]], dtype=int8),\n",
       " 3: memmap([[1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0]], dtype=int8),\n",
       " 4: memmap([[1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0]], dtype=int8)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b334fde5ebc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0m_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msetnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "setnum = 0\n",
    "index = 0\n",
    "\n",
    "col_start = {\"acc\": 0, \"rot\": 3}\n",
    "col = col_start['rot']\n",
    "X, y = train['X'], train['y']\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X = train['X']\n",
    "y[:,0] == 0\n",
    "_X = X[setnum][index]\n",
    "plt.plot(_X[:,col])\n",
    "plt.plot(_X[:,col+1])\n",
    "plt.plot(_X[:,col+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 254, 64)           1216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 254, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 254, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 127, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 125, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 125, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 62, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 60, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 60, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 28, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 28, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 28, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 14, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14, 32)            2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 14, 32)            128       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 1347      \n",
      "=================================================================\n",
      "Total params: 59,491\n",
      "Trainable params: 58,915\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_timesteps = 256\n",
    "n_features = 6\n",
    "n_outputs = 3\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# First CNN layer\n",
    "model.add(layers.Conv1D(kernel_size=3, \n",
    "                        filters=64, \n",
    "                        activation='relu', \n",
    "                        input_shape=(n_timesteps, n_features)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "# Second CNN layer\n",
    "model.add(layers.Conv1D(kernel_size = 3, \n",
    "                        filters = 64, \n",
    "                        activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "\n",
    "# Third CNN layer\n",
    "model.add(layers.Conv1D(kernel_size = 3, \n",
    "                        filters = 64, \n",
    "                        activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "# Fourth CNN layer\n",
    "model.add(layers.Conv1D(kernel_size = 3, \n",
    "                        filters = 64, \n",
    "                        activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "model.add(layers.Dense(32))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.LSTM(32, return_sequences=True))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.LSTM(32, return_sequences=True))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(3))\n",
    "model.add(layers.Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1149/1149 [==============================] - 47s 41ms/step - loss: 0.1083 - accuracy: 0.9743\n",
      "Epoch 2/100\n",
      "1149/1149 [==============================] - 48s 41ms/step - loss: 0.0698 - accuracy: 0.9790\n",
      "Epoch 3/100\n",
      "1149/1149 [==============================] - 47s 41ms/step - loss: 0.0619 - accuracy: 0.9810\n",
      "Epoch 4/100\n",
      "1149/1149 [==============================] - 47s 41ms/step - loss: 0.0556 - accuracy: 0.9818\n",
      "Epoch 5/100\n",
      "1149/1149 [==============================] - 47s 41ms/step - loss: 0.0509 - accuracy: 0.9834\n",
      "Epoch 6/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0484 - accuracy: 0.9839\n",
      "Epoch 7/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0458 - accuracy: 0.9849\n",
      "Epoch 8/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0435 - accuracy: 0.9854\n",
      "Epoch 9/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0420 - accuracy: 0.9859\n",
      "Epoch 10/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0418 - accuracy: 0.9858\n",
      "Epoch 11/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0385 - accuracy: 0.9869\n",
      "Epoch 12/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0376 - accuracy: 0.9875\n",
      "Epoch 13/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0354 - accuracy: 0.9877\n",
      "Epoch 14/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0358 - accuracy: 0.9879\n",
      "Epoch 15/100\n",
      "1149/1149 [==============================] - 45s 40ms/step - loss: 0.0339 - accuracy: 0.9884\n",
      "Epoch 16/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0328 - accuracy: 0.9887\n",
      "Epoch 17/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0307 - accuracy: 0.9894\n",
      "Epoch 18/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0300 - accuracy: 0.9895\n",
      "Epoch 19/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0309 - accuracy: 0.9896\n",
      "Epoch 20/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0291 - accuracy: 0.9898\n",
      "Epoch 21/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0277 - accuracy: 0.9903\n",
      "Epoch 22/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0276 - accuracy: 0.9907\n",
      "Epoch 23/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0276 - accuracy: 0.9905\n",
      "Epoch 24/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0251 - accuracy: 0.9913\n",
      "Epoch 25/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0251 - accuracy: 0.9915\n",
      "Epoch 26/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0241 - accuracy: 0.9915\n",
      "Epoch 27/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0248 - accuracy: 0.9917\n",
      "Epoch 28/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0242 - accuracy: 0.9916\n",
      "Epoch 29/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0231 - accuracy: 0.9920\n",
      "Epoch 30/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0227 - accuracy: 0.9923\n",
      "Epoch 31/100\n",
      "1149/1149 [==============================] - 47s 41ms/step - loss: 0.0219 - accuracy: 0.9923\n",
      "Epoch 32/100\n",
      "1149/1149 [==============================] - 47s 41ms/step - loss: 0.0227 - accuracy: 0.9919\n",
      "Epoch 33/100\n",
      "1149/1149 [==============================] - 47s 41ms/step - loss: 0.0208 - accuracy: 0.9928\n",
      "Epoch 34/100\n",
      "1149/1149 [==============================] - 702s 611ms/step - loss: 0.0197 - accuracy: 0.9930\n",
      "Epoch 35/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0207 - accuracy: 0.9926\n",
      "Epoch 36/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0201 - accuracy: 0.9931\n",
      "Epoch 37/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0183 - accuracy: 0.9933\n",
      "Epoch 38/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0192 - accuracy: 0.9934\n",
      "Epoch 39/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0186 - accuracy: 0.9937\n",
      "Epoch 40/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0176 - accuracy: 0.9938\n",
      "Epoch 41/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0162 - accuracy: 0.9943\n",
      "Epoch 42/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0177 - accuracy: 0.9938\n",
      "Epoch 43/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0171 - accuracy: 0.9941\n",
      "Epoch 44/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0167 - accuracy: 0.9941\n",
      "Epoch 45/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0174 - accuracy: 0.9940\n",
      "Epoch 46/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0163 - accuracy: 0.9945\n",
      "Epoch 47/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0162 - accuracy: 0.9944\n",
      "Epoch 48/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0157 - accuracy: 0.9945\n",
      "Epoch 49/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0150 - accuracy: 0.9948\n",
      "Epoch 50/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0146 - accuracy: 0.9949\n",
      "Epoch 51/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0144 - accuracy: 0.9947\n",
      "Epoch 52/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0136 - accuracy: 0.9955\n",
      "Epoch 53/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0145 - accuracy: 0.9950\n",
      "Epoch 54/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0139 - accuracy: 0.9947\n",
      "Epoch 55/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0141 - accuracy: 0.9951\n",
      "Epoch 56/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0137 - accuracy: 0.9952\n",
      "Epoch 57/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0134 - accuracy: 0.9953\n",
      "Epoch 58/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0133 - accuracy: 0.9953\n",
      "Epoch 59/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0128 - accuracy: 0.9954\n",
      "Epoch 60/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0117 - accuracy: 0.9957\n",
      "Epoch 61/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0135 - accuracy: 0.9952\n",
      "Epoch 62/100\n",
      "1149/1149 [==============================] - 45s 40ms/step - loss: 0.0132 - accuracy: 0.9954\n",
      "Epoch 63/100\n",
      "1149/1149 [==============================] - 46s 40ms/step - loss: 0.0111 - accuracy: 0.9959\n",
      "Epoch 64/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0121 - accuracy: 0.9957\n",
      "Epoch 65/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0120 - accuracy: 0.9959\n",
      "Epoch 66/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0112 - accuracy: 0.9960\n",
      "Epoch 67/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0117 - accuracy: 0.9960\n",
      "Epoch 68/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0110 - accuracy: 0.9961\n",
      "Epoch 69/100\n",
      "1149/1149 [==============================] - 43s 38ms/step - loss: 0.0119 - accuracy: 0.9959\n",
      "Epoch 70/100\n",
      "1149/1149 [==============================] - 43s 38ms/step - loss: 0.0110 - accuracy: 0.9962\n",
      "Epoch 71/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0108 - accuracy: 0.9964\n",
      "Epoch 72/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0104 - accuracy: 0.9965\n",
      "Epoch 73/100\n",
      "1149/1149 [==============================] - 43s 38ms/step - loss: 0.0106 - accuracy: 0.9964\n",
      "Epoch 74/100\n",
      "1149/1149 [==============================] - 43s 38ms/step - loss: 0.0111 - accuracy: 0.9960\n",
      "Epoch 75/100\n",
      "1149/1149 [==============================] - 43s 38ms/step - loss: 0.0107 - accuracy: 0.9965\n",
      "Epoch 76/100\n",
      "1149/1149 [==============================] - 43s 38ms/step - loss: 0.0108 - accuracy: 0.9964\n",
      "Epoch 77/100\n",
      "1149/1149 [==============================] - 43s 38ms/step - loss: 0.0091 - accuracy: 0.9966\n",
      "Epoch 78/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0097 - accuracy: 0.9964\n",
      "Epoch 79/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0096 - accuracy: 0.9966\n",
      "Epoch 80/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0099 - accuracy: 0.9965\n",
      "Epoch 81/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0094 - accuracy: 0.9969\n",
      "Epoch 82/100\n",
      "1149/1149 [==============================] - 44s 38ms/step - loss: 0.0092 - accuracy: 0.9967\n",
      "Epoch 83/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0088 - accuracy: 0.9969\n",
      "Epoch 84/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0098 - accuracy: 0.9965\n",
      "Epoch 85/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0099 - accuracy: 0.9964\n",
      "Epoch 86/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0080 - accuracy: 0.9973\n",
      "Epoch 87/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0089 - accuracy: 0.9971\n",
      "Epoch 88/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0088 - accuracy: 0.9970\n",
      "Epoch 89/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0087 - accuracy: 0.9971\n",
      "Epoch 90/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0089 - accuracy: 0.9969\n",
      "Epoch 91/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0078 - accuracy: 0.9972\n",
      "Epoch 92/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0084 - accuracy: 0.9970\n",
      "Epoch 93/100\n",
      "1149/1149 [==============================] - 44s 39ms/step - loss: 0.0094 - accuracy: 0.9969\n",
      "Epoch 94/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0078 - accuracy: 0.9971\n",
      "Epoch 95/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0075 - accuracy: 0.9974\n",
      "Epoch 96/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0074 - accuracy: 0.9973\n",
      "Epoch 97/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0073 - accuracy: 0.9977\n",
      "Epoch 98/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0080 - accuracy: 0.9973\n",
      "Epoch 99/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0083 - accuracy: 0.9970\n",
      "Epoch 100/100\n",
      "1149/1149 [==============================] - 45s 39ms/step - loss: 0.0081 - accuracy: 0.9974\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "X_train, y_train = train['X'][i], train['y'][i]\n",
    "X_test,  y_test  =  test['X'][i],  test['y'][i]\n",
    "history = model.fit(X_train, y_train, epochs = 100, batch_size = 64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643/643 - 6s - loss: 0.1079 - accuracy: 0.9842\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1d202f4f90>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "# plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_max = np.argmax(y_pred, axis=1)\n",
    "y_test_max = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20047,    72,    50],\n",
       "       [  150,    48,     4],\n",
       "       [   47,     1,   136]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "confusion_matrix(y_test_max, y_pred_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no fall       0.99      0.99      0.99     20169\n",
      "    pre-fall       0.40      0.24      0.30       202\n",
      "        fall       0.72      0.74      0.73       184\n",
      "\n",
      "    accuracy                           0.98     20555\n",
      "   macro avg       0.70      0.66      0.67     20555\n",
      "weighted avg       0.98      0.98      0.98     20555\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_max, y_pred_max, target_names=['no fall', 'pre-fall', 'fall']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for classification\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Sensitivity} &= \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \\\\\n",
    "\\text{Specificity} &= \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} \\\\\n",
    "\\text{Accuracy} &= \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{TN} + \\text{FN}} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Label 0\n",
      "    accuracy\t0.984\n",
      " specificity\t0.490\n",
      " sensitivity\t0.994\n",
      " Label 1\n",
      "    accuracy\t0.989\n",
      " specificity\t0.996\n",
      " sensitivity\t0.238\n",
      " Label 2\n",
      "    accuracy\t0.995\n",
      " specificity\t0.997\n",
      " sensitivity\t0.739\n"
     ]
    }
   ],
   "source": [
    "def truth_test(_test,_pred, i):\n",
    "    _test = np.array(_test)\n",
    "    _pred = np.array(_pred)\n",
    "    \n",
    "    _pred_pos = _test[_pred == i]\n",
    "    _pred_neg = _test[_pred != i]\n",
    "    \n",
    "    _true_pos = len(_pred_pos[_pred_pos == i])\n",
    "    _fals_pos = len(_pred_pos[_pred_pos != i])\n",
    "    \n",
    "    _true_neg = len(_pred_neg[_pred_neg != i])\n",
    "    _fals_neg = len(_pred_neg[_pred_neg == i])\n",
    "    \n",
    "    return _true_pos, _fals_pos, _true_neg, _fals_neg\n",
    "\n",
    "def sensitivity(_test,_pred, i):\n",
    "    tp, fp, tn, fn = truth_test(_test, _pred, i)\n",
    "    return tp / ( tp + fn)\n",
    "\n",
    "def specificity(_test,_pred, i):\n",
    "    tp, fp, tn, fn = truth_test(_test, _pred, i)\n",
    "    return tn / ( tn + fp)\n",
    "\n",
    "def accuracy(_test, _pred, i):\n",
    "    tp, fp, tn, fn = truth_test(_test, _pred, i)\n",
    "    return (tp+tn) / (tp + fp + tn + fn)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\" Label\", i)\n",
    "    print(\"    accuracy\\t%5.3f\"%accuracy(y_test_max, y_pred_max, i))\n",
    "    print(\" specificity\\t%5.3f\"%specificity(y_test_max, y_pred_max, i))\n",
    "    print(\" sensitivity\\t%5.3f\"%sensitivity(y_test_max, y_pred_max, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "import h5py\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_100.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_100.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "accuracy: 98.50%\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1d202749d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow([[20047,    72,    50],\n",
    "            [  150,    48,     4],\n",
    "            [   47,     1,   136]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
