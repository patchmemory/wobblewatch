{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# this is for loading the data\n",
    "def load_fall_X(file_name):\n",
    "    temp = np.memmap(file_name, dtype='float32', mode='r')\n",
    "    X = np.reshape(temp, [-1, 256, 6])\n",
    "    return X\n",
    "\n",
    "# this is for loading the labels (one-hot encoding: [1, 0, 0]-->nonfall, [0, 1, 0]-->pre-impact fall, [0, 0, 1]-->fall\t\n",
    "def load_fall_y(file_name):\n",
    "    temp = np.memmap(file_name, dtype='int8', mode='r')\n",
    "    y= np.reshape(temp, [-1, 3])\n",
    "    return y\n",
    "\n",
    "def dset_fpath(fname):\n",
    "    _fp = os.path.abspath('.')\n",
    "    _hp = \"wobblewatch\"\n",
    "    _fp = _fp.split(_hp)[0]\n",
    "    return os.path.abspath(\"%s/%s/data/sisfall/preproc/%s\" % (_fp, _hp, fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test  = {'X': {}, 'y': {}}\n",
    "train = {'X': {}, 'y': {}}\n",
    "for i in range(5):\n",
    "    test['X'][i] = load_fall_X(dset_fpath(\"test_x_%i\" % i))\n",
    "    test['y'][i] = load_fall_y(dset_fpath(\"test_y_%i\" % i))\n",
    "    train['X'][i] = load_fall_X(dset_fpath(\"train_x_%i\" % i))\n",
    "    train['y'][i] = load_fall_y(dset_fpath(\"train_y_%i\" % i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 0\n",
      " X train shape: (75025, 256, 6)\n",
      " y train shape: (75025, 3)\n",
      " X test shape:  (19064, 256, 6)\n",
      " y test shape:  (19064, 3)\n",
      "Set 1\n",
      " X train shape: (73347, 256, 6)\n",
      " y train shape: (73347, 3)\n",
      " X test shape:  (20742, 256, 6)\n",
      " y test shape:  (20742, 3)\n",
      "Set 2\n",
      " X train shape: (73534, 256, 6)\n",
      " y train shape: (73534, 3)\n",
      " X test shape:  (20555, 256, 6)\n",
      " y test shape:  (20555, 3)\n",
      "Set 3\n",
      " X train shape: (77296, 256, 6)\n",
      " y train shape: (77296, 3)\n",
      " X test shape:  (16793, 256, 6)\n",
      " y test shape:  (16793, 3)\n",
      "Set 4\n",
      " X train shape: (77154, 256, 6)\n",
      " y train shape: (77154, 3)\n",
      " X test shape:  (16935, 256, 6)\n",
      " y test shape:  (16935, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Set\", i)\n",
    "    print(\" X train shape:\", train['X'][i].shape)\n",
    "    print(\" y train shape:\", train['y'][i].shape)\n",
    "    print(\" X test shape: \", test['X'][i].shape)\n",
    "    print(\" y test shape: \", test['y'][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['y'][0]\n",
    "print(y)\n",
    "y[y[:,0] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "setnum = 0\n",
    "\n",
    "\n",
    "col_start = {\"acc\": 0, \"rot\": 3}\n",
    "col = col_start['rot']\n",
    "X, y = train['X'][setnum], train['y'][setnum]\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(X.shape, y.shape)\n",
    "labeli = 1\n",
    "labelj = 1\n",
    "X_fpf, y_fpf = X[y[:,labeli] == labelj], y[y[:,labeli] == labelj]\n",
    "print(X_fpf.shape, y_fpf.shape)\n",
    "\n",
    "# for index in range(len(X_fpf)):\n",
    "for index in range(5):\n",
    "    _X = X[index]\n",
    "#     print(_X.shape)\n",
    "    plt.plot(_X[:,col])\n",
    "    plt.plot(_X[:,col+1])\n",
    "    plt.plot(_X[:,col+2])\n",
    "    plt.show()\n",
    "    \n",
    "X_fpf, y_fpf = X[y[:,0] == 0], y[y[:,0] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "from tensorflow_addons import losses \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n_timesteps = 256\n",
    "n_features = 6\n",
    "n_outputs = 3\n",
    "\n",
    "def hybrid_model():\n",
    "    model = models.Sequential()\n",
    "    # First CNN layer\n",
    "    model.add(layers.Conv1D(kernel_size=3, \n",
    "                            filters=64, \n",
    "                            activation='relu', \n",
    "                            input_shape=(n_timesteps, n_features)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    # Second CNN layer\n",
    "    model.add(layers.Conv1D(kernel_size = 3, \n",
    "                            filters = 64, \n",
    "                            activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    # Third CNN layer\n",
    "    model.add(layers.Conv1D(kernel_size = 3, \n",
    "                            filters = 64, \n",
    "                            activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    # Fourth CNN layer\n",
    "    model.add(layers.Conv1D(kernel_size = 3, \n",
    "                            filters = 64, \n",
    "                            activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    # LSTM Layers\n",
    "    model.add(layers.Dense(32))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    # First LSTM\n",
    "    model.add(layers.LSTM(32, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    # Second LSTM\n",
    "    model.add(layers.LSTM(32, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(n_outputs, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    opt = optimizers.Adam(learning_rate=0.0005)\n",
    "    lss = losses.SigmoidFocalCrossEntropy()\n",
    "    model.compile(loss=lss, optimizer=opt, metrics=['accuracy'])    \n",
    "\n",
    "#     model.compile(loss=focal_loss(alpha=1), optimizer='nadam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next I run the 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 254, 64)           1216      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 254, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 254, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 127, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 125, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 125, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 62, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 60, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 60, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 28, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 28, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 14, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 14, 32)            2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 32)            128       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 1347      \n",
      "=================================================================\n",
      "Total params: 59,491\n",
      "Trainable params: 58,915\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "1173/1173 [==============================] - 51s 43ms/step - loss: 0.0244 - accuracy: 0.9729 - val_loss: 0.0171 - val_accuracy: 0.9804\n",
      "Epoch 2/20\n",
      "1173/1173 [==============================] - 50s 43ms/step - loss: 0.0172 - accuracy: 0.9788 - val_loss: 0.0157 - val_accuracy: 0.9770\n",
      "Epoch 3/20\n",
      "1173/1173 [==============================] - 49s 42ms/step - loss: 0.0151 - accuracy: 0.9798 - val_loss: 0.0146 - val_accuracy: 0.9816\n",
      "Epoch 4/20\n",
      "1173/1173 [==============================] - 58s 49ms/step - loss: 0.0135 - accuracy: 0.9809 - val_loss: 0.0121 - val_accuracy: 0.9832\n",
      "Epoch 5/20\n",
      "1173/1173 [==============================] - 70s 60ms/step - loss: 0.0126 - accuracy: 0.9816 - val_loss: 0.0120 - val_accuracy: 0.9840\n",
      "Epoch 6/20\n",
      "1173/1173 [==============================] - 70s 60ms/step - loss: 0.0118 - accuracy: 0.9824 - val_loss: 0.0120 - val_accuracy: 0.9845\n",
      "Epoch 7/20\n",
      "1173/1173 [==============================] - 68s 58ms/step - loss: 0.0112 - accuracy: 0.9832 - val_loss: 0.0113 - val_accuracy: 0.9842\n",
      "Epoch 8/20\n",
      "1173/1173 [==============================] - 69s 58ms/step - loss: 0.0105 - accuracy: 0.9837 - val_loss: 0.0132 - val_accuracy: 0.9848\n",
      "Epoch 9/20\n",
      "1173/1173 [==============================] - 70s 59ms/step - loss: 0.0100 - accuracy: 0.9843 - val_loss: 0.0124 - val_accuracy: 0.9838\n",
      "Epoch 10/20\n",
      "1173/1173 [==============================] - 68s 58ms/step - loss: 0.0099 - accuracy: 0.9850 - val_loss: 0.0120 - val_accuracy: 0.9850\n",
      "Epoch 11/20\n",
      "1173/1173 [==============================] - 71s 61ms/step - loss: 0.0095 - accuracy: 0.9850 - val_loss: 0.0127 - val_accuracy: 0.9853\n",
      "Epoch 12/20\n",
      "1173/1173 [==============================] - 68s 58ms/step - loss: 0.0091 - accuracy: 0.9857 - val_loss: 0.0135 - val_accuracy: 0.9846\n",
      "Epoch 13/20\n",
      "1173/1173 [==============================] - 70s 60ms/step - loss: 0.0088 - accuracy: 0.9857 - val_loss: 0.0173 - val_accuracy: 0.9855\n",
      "Epoch 14/20\n",
      "1173/1173 [==============================] - 72s 61ms/step - loss: 0.0085 - accuracy: 0.9867 - val_loss: 0.0156 - val_accuracy: 0.9848\n",
      "Epoch 15/20\n",
      "1173/1173 [==============================] - 72s 62ms/step - loss: 0.0078 - accuracy: 0.9872 - val_loss: 0.0110 - val_accuracy: 0.9861\n",
      "Epoch 16/20\n",
      "1173/1173 [==============================] - 72s 61ms/step - loss: 0.0080 - accuracy: 0.9876 - val_loss: 0.0112 - val_accuracy: 0.9857\n",
      "Epoch 17/20\n",
      "1173/1173 [==============================] - 71s 60ms/step - loss: 0.0076 - accuracy: 0.9879 - val_loss: 0.0145 - val_accuracy: 0.9860\n",
      "Epoch 18/20\n",
      "1173/1173 [==============================] - 72s 62ms/step - loss: 0.0075 - accuracy: 0.9884 - val_loss: 0.0135 - val_accuracy: 0.9856\n",
      "Epoch 19/20\n",
      "1173/1173 [==============================] - 72s 62ms/step - loss: 0.0076 - accuracy: 0.9883 - val_loss: 0.0125 - val_accuracy: 0.9809\n",
      "Epoch 20/20\n",
      "1173/1173 [==============================] - 72s 61ms/step - loss: 0.0071 - accuracy: 0.9888 - val_loss: 0.0151 - val_accuracy: 0.9856\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 254, 64)           1216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 254, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 254, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 127, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 125, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 125, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 62, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 60, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 60, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 28, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 28, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 28, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 14, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14, 32)            2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 14, 32)            128       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 1347      \n",
      "=================================================================\n",
      "Total params: 59,491\n",
      "Trainable params: 58,915\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1147/1147 [==============================] - 72s 62ms/step - loss: 0.0226 - accuracy: 0.9723 - val_loss: 0.0191 - val_accuracy: 0.9766\n",
      "Epoch 2/20\n",
      "1147/1147 [==============================] - 71s 62ms/step - loss: 0.0159 - accuracy: 0.9793 - val_loss: 0.0162 - val_accuracy: 0.9797\n",
      "Epoch 3/20\n",
      "1147/1147 [==============================] - 70s 61ms/step - loss: 0.0136 - accuracy: 0.9806 - val_loss: 0.0187 - val_accuracy: 0.9701\n",
      "Epoch 4/20\n",
      "1147/1147 [==============================] - 70s 61ms/step - loss: 0.0125 - accuracy: 0.9815 - val_loss: 0.0149 - val_accuracy: 0.9799\n",
      "Epoch 5/20\n",
      "1147/1147 [==============================] - 71s 62ms/step - loss: 0.0117 - accuracy: 0.9826 - val_loss: 0.0161 - val_accuracy: 0.9763\n",
      "Epoch 6/20\n",
      "1147/1147 [==============================] - 70s 61ms/step - loss: 0.0112 - accuracy: 0.9827 - val_loss: 0.0155 - val_accuracy: 0.9811\n",
      "Epoch 7/20\n",
      "1147/1147 [==============================] - 69s 60ms/step - loss: 0.0105 - accuracy: 0.9835 - val_loss: 0.0151 - val_accuracy: 0.9812\n",
      "Epoch 8/20\n",
      "1147/1147 [==============================] - 71s 62ms/step - loss: 0.0100 - accuracy: 0.9843 - val_loss: 0.0157 - val_accuracy: 0.9809\n",
      "Epoch 9/20\n",
      "1147/1147 [==============================] - 71s 62ms/step - loss: 0.0096 - accuracy: 0.9848 - val_loss: 0.0163 - val_accuracy: 0.9756\n",
      "Epoch 10/20\n",
      "1147/1147 [==============================] - 69s 60ms/step - loss: 0.0094 - accuracy: 0.9857 - val_loss: 0.0155 - val_accuracy: 0.9805\n",
      "Epoch 11/20\n",
      "1147/1147 [==============================] - 70s 61ms/step - loss: 0.0090 - accuracy: 0.9862 - val_loss: 0.0148 - val_accuracy: 0.9790\n",
      "Epoch 12/20\n",
      "1147/1147 [==============================] - 68s 59ms/step - loss: 0.0088 - accuracy: 0.9870 - val_loss: 0.0171 - val_accuracy: 0.9805\n",
      "Epoch 13/20\n",
      "1147/1147 [==============================] - 68s 59ms/step - loss: 0.0085 - accuracy: 0.9862 - val_loss: 0.0201 - val_accuracy: 0.9687\n",
      "Epoch 14/20\n",
      "1147/1147 [==============================] - 68s 60ms/step - loss: 0.0082 - accuracy: 0.9871 - val_loss: 0.0186 - val_accuracy: 0.9814\n",
      "Epoch 15/20\n",
      "1147/1147 [==============================] - 69s 60ms/step - loss: 0.0085 - accuracy: 0.9870 - val_loss: 0.0160 - val_accuracy: 0.9798\n",
      "Epoch 16/20\n",
      "1147/1147 [==============================] - 68s 59ms/step - loss: 0.0080 - accuracy: 0.9873 - val_loss: 0.0181 - val_accuracy: 0.9771\n",
      "Epoch 17/20\n",
      "1147/1147 [==============================] - 69s 60ms/step - loss: 0.0079 - accuracy: 0.9881 - val_loss: 0.0166 - val_accuracy: 0.9795\n",
      "Epoch 18/20\n",
      "1147/1147 [==============================] - 69s 60ms/step - loss: 0.0074 - accuracy: 0.9882 - val_loss: 0.0169 - val_accuracy: 0.9799\n",
      "Epoch 19/20\n",
      "1147/1147 [==============================] - 69s 60ms/step - loss: 0.0070 - accuracy: 0.9891 - val_loss: 0.0174 - val_accuracy: 0.9768\n",
      "Epoch 20/20\n",
      "1147/1147 [==============================] - 66s 58ms/step - loss: 0.0073 - accuracy: 0.9886 - val_loss: 0.0150 - val_accuracy: 0.9809\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_8 (Conv1D)            (None, 254, 64)           1216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 254, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 254, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 127, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 125, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 125, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 62, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 60, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 60, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 28, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 28, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_11 (ReLU)              (None, 28, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 14, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 14, 32)            2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 14, 32)            128       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 1347      \n",
      "=================================================================\n",
      "Total params: 59,491\n",
      "Trainable params: 58,915\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "1149/1149 [==============================] - 69s 60ms/step - loss: 0.0251 - accuracy: 0.9707 - val_loss: 0.0150 - val_accuracy: 0.9813\n",
      "Epoch 2/20\n",
      "1149/1149 [==============================] - 68s 59ms/step - loss: 0.0174 - accuracy: 0.9785 - val_loss: 0.0137 - val_accuracy: 0.9811\n",
      "Epoch 3/20\n",
      "1149/1149 [==============================] - 68s 59ms/step - loss: 0.0149 - accuracy: 0.9793 - val_loss: 0.0141 - val_accuracy: 0.9820\n",
      "Epoch 4/20\n",
      "1149/1149 [==============================] - 69s 60ms/step - loss: 0.0131 - accuracy: 0.9809 - val_loss: 0.0130 - val_accuracy: 0.9824\n",
      "Epoch 5/20\n",
      "1149/1149 [==============================] - 70s 61ms/step - loss: 0.0124 - accuracy: 0.9816 - val_loss: 0.0120 - val_accuracy: 0.9835\n",
      "Epoch 6/20\n",
      "1149/1149 [==============================] - 68s 59ms/step - loss: 0.0114 - accuracy: 0.9825 - val_loss: 0.0111 - val_accuracy: 0.9838\n",
      "Epoch 7/20\n",
      "1149/1149 [==============================] - 68s 59ms/step - loss: 0.0109 - accuracy: 0.9827 - val_loss: 0.0137 - val_accuracy: 0.9837\n",
      "Epoch 8/20\n",
      "1149/1149 [==============================] - 70s 61ms/step - loss: 0.0105 - accuracy: 0.9837 - val_loss: 0.0143 - val_accuracy: 0.9846\n",
      "Epoch 9/20\n",
      "1149/1149 [==============================] - 68s 59ms/step - loss: 0.0099 - accuracy: 0.9845 - val_loss: 0.0116 - val_accuracy: 0.9837\n",
      "Epoch 10/20\n",
      "1149/1149 [==============================] - 69s 60ms/step - loss: 0.0096 - accuracy: 0.9850 - val_loss: 0.0140 - val_accuracy: 0.9825\n",
      "Epoch 11/20\n",
      "1149/1149 [==============================] - 69s 60ms/step - loss: 0.0096 - accuracy: 0.9849 - val_loss: 0.0139 - val_accuracy: 0.9856\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1149/1149 [==============================] - 66s 57ms/step - loss: 0.0090 - accuracy: 0.9854 - val_loss: 0.0150 - val_accuracy: 0.9856\n",
      "Epoch 13/20\n",
      "1149/1149 [==============================] - 70s 60ms/step - loss: 0.0088 - accuracy: 0.9859 - val_loss: 0.0147 - val_accuracy: 0.9854\n",
      "Epoch 14/20\n",
      "1149/1149 [==============================] - 69s 60ms/step - loss: 0.0087 - accuracy: 0.9864 - val_loss: 0.0126 - val_accuracy: 0.9847\n",
      "Epoch 15/20\n",
      "1149/1149 [==============================] - 67s 58ms/step - loss: 0.0082 - accuracy: 0.9866 - val_loss: 0.0149 - val_accuracy: 0.9843\n",
      "Epoch 16/20\n",
      "1149/1149 [==============================] - 69s 60ms/step - loss: 0.0083 - accuracy: 0.9870 - val_loss: 0.0124 - val_accuracy: 0.9858\n",
      "Epoch 17/20\n",
      "1149/1149 [==============================] - 67s 58ms/step - loss: 0.0079 - accuracy: 0.9880 - val_loss: 0.0141 - val_accuracy: 0.9834\n",
      "Epoch 18/20\n",
      "1149/1149 [==============================] - 68s 59ms/step - loss: 0.0075 - accuracy: 0.9878 - val_loss: 0.0167 - val_accuracy: 0.9799\n",
      "Epoch 19/20\n",
      "1149/1149 [==============================] - 67s 59ms/step - loss: 0.0075 - accuracy: 0.9877 - val_loss: 0.0149 - val_accuracy: 0.9822\n",
      "Epoch 20/20\n",
      "1149/1149 [==============================] - 68s 59ms/step - loss: 0.0072 - accuracy: 0.9883 - val_loss: 0.0140 - val_accuracy: 0.9803\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 254, 64)           1216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 254, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu_12 (ReLU)              (None, 254, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 127, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 125, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 125, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu_13 (ReLU)              (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 62, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 60, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 60, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_14 (ReLU)              (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 28, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 28, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_15 (ReLU)              (None, 28, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 14, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 14, 32)            2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 14, 32)            128       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 1347      \n",
      "=================================================================\n",
      "Total params: 59,491\n",
      "Trainable params: 58,915\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "1208/1208 [==============================] - 72s 60ms/step - loss: 0.0234 - accuracy: 0.9724 - val_loss: 0.0207 - val_accuracy: 0.9785\n",
      "Epoch 2/20\n",
      "1208/1208 [==============================] - 71s 59ms/step - loss: 0.0164 - accuracy: 0.9798 - val_loss: 0.0149 - val_accuracy: 0.9809\n",
      "Epoch 3/20\n",
      "1208/1208 [==============================] - 70s 58ms/step - loss: 0.0143 - accuracy: 0.9804 - val_loss: 0.0165 - val_accuracy: 0.9818\n",
      "Epoch 4/20\n",
      "1208/1208 [==============================] - 51s 42ms/step - loss: 0.0129 - accuracy: 0.9814 - val_loss: 0.0155 - val_accuracy: 0.9828\n",
      "Epoch 5/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0124 - accuracy: 0.9817 - val_loss: 0.0125 - val_accuracy: 0.9827\n",
      "Epoch 6/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0111 - accuracy: 0.9827 - val_loss: 0.0134 - val_accuracy: 0.9825\n",
      "Epoch 7/20\n",
      "1208/1208 [==============================] - 50s 41ms/step - loss: 0.0106 - accuracy: 0.9835 - val_loss: 0.0136 - val_accuracy: 0.9841\n",
      "Epoch 8/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0102 - accuracy: 0.9843 - val_loss: 0.0144 - val_accuracy: 0.9848\n",
      "Epoch 9/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0096 - accuracy: 0.9848 - val_loss: 0.0126 - val_accuracy: 0.9860\n",
      "Epoch 10/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0090 - accuracy: 0.9852 - val_loss: 0.0190 - val_accuracy: 0.9840\n",
      "Epoch 11/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0089 - accuracy: 0.9856 - val_loss: 0.0130 - val_accuracy: 0.9818\n",
      "Epoch 12/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0086 - accuracy: 0.9864 - val_loss: 0.0138 - val_accuracy: 0.9856\n",
      "Epoch 13/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0083 - accuracy: 0.9870 - val_loss: 0.0127 - val_accuracy: 0.9835\n",
      "Epoch 14/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0081 - accuracy: 0.9870 - val_loss: 0.0134 - val_accuracy: 0.9812\n",
      "Epoch 15/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0079 - accuracy: 0.9879 - val_loss: 0.0114 - val_accuracy: 0.9855\n",
      "Epoch 16/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0074 - accuracy: 0.9876 - val_loss: 0.0115 - val_accuracy: 0.9857\n",
      "Epoch 17/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0071 - accuracy: 0.9881 - val_loss: 0.0188 - val_accuracy: 0.9856\n",
      "Epoch 18/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0075 - accuracy: 0.9882 - val_loss: 0.0114 - val_accuracy: 0.9859\n",
      "Epoch 19/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0071 - accuracy: 0.9889 - val_loss: 0.0142 - val_accuracy: 0.9864\n",
      "Epoch 20/20\n",
      "1208/1208 [==============================] - 49s 41ms/step - loss: 0.0069 - accuracy: 0.9892 - val_loss: 0.0128 - val_accuracy: 0.9837\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_16 (Conv1D)           (None, 254, 64)           1216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 254, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu_16 (ReLU)              (None, 254, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 127, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 125, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 125, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu_17 (ReLU)              (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 62, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 60, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 60, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_18 (ReLU)              (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 28, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 28, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_19 (ReLU)              (None, 28, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 14, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 14, 32)            2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 14, 32)            128       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 14, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 1347      \n",
      "=================================================================\n",
      "Total params: 59,491\n",
      "Trainable params: 58,915\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206/1206 [==============================] - 50s 42ms/step - loss: 0.0234 - accuracy: 0.9733 - val_loss: 0.0160 - val_accuracy: 0.9796\n",
      "Epoch 2/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0160 - accuracy: 0.9795 - val_loss: 0.0213 - val_accuracy: 0.9661\n",
      "Epoch 3/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0138 - accuracy: 0.9801 - val_loss: 0.0157 - val_accuracy: 0.9783\n",
      "Epoch 4/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0125 - accuracy: 0.9811 - val_loss: 0.0149 - val_accuracy: 0.9765\n",
      "Epoch 5/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0118 - accuracy: 0.9822 - val_loss: 0.0161 - val_accuracy: 0.9745\n",
      "Epoch 6/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0110 - accuracy: 0.9833 - val_loss: 0.0240 - val_accuracy: 0.9559\n",
      "Epoch 7/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0107 - accuracy: 0.9837 - val_loss: 0.0136 - val_accuracy: 0.9810\n",
      "Epoch 8/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0102 - accuracy: 0.9844 - val_loss: 0.0131 - val_accuracy: 0.9806\n",
      "Epoch 9/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0097 - accuracy: 0.9849 - val_loss: 0.0158 - val_accuracy: 0.9762\n",
      "Epoch 10/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0093 - accuracy: 0.9856 - val_loss: 0.0153 - val_accuracy: 0.9779\n",
      "Epoch 11/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0091 - accuracy: 0.9861 - val_loss: 0.0153 - val_accuracy: 0.9803\n",
      "Epoch 12/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0089 - accuracy: 0.9860 - val_loss: 0.0153 - val_accuracy: 0.9770\n",
      "Epoch 13/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0084 - accuracy: 0.9870 - val_loss: 0.0185 - val_accuracy: 0.9756\n",
      "Epoch 14/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0085 - accuracy: 0.9867 - val_loss: 0.0200 - val_accuracy: 0.9795\n",
      "Epoch 15/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0080 - accuracy: 0.9875 - val_loss: 0.0224 - val_accuracy: 0.9696\n",
      "Epoch 16/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0077 - accuracy: 0.9879 - val_loss: 0.0232 - val_accuracy: 0.9724\n",
      "Epoch 17/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0076 - accuracy: 0.9881 - val_loss: 0.0145 - val_accuracy: 0.9822\n",
      "Epoch 18/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0075 - accuracy: 0.9884 - val_loss: 0.0157 - val_accuracy: 0.9820\n",
      "Epoch 19/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0072 - accuracy: 0.9890 - val_loss: 0.0145 - val_accuracy: 0.9810\n",
      "Epoch 20/20\n",
      "1206/1206 [==============================] - 49s 41ms/step - loss: 0.0070 - accuracy: 0.9893 - val_loss: 0.0165 - val_accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "history = {}\n",
    "mod = {}\n",
    "for fold in range(n_folds):\n",
    "    X_train, y_train = train['X'][fold], train['y'][fold]\n",
    "    X_test,  y_test  =  test['X'][fold],  test['y'][fold]\n",
    "    mod[fold] = hybrid_model()\n",
    "    history[fold] = mod[fold].fit(train['X'][fold], train['y'][fold], \n",
    "                                  validation_data=(test['X'][fold], test['y'][fold]), \n",
    "                                  epochs = n_epochs, batch_size = batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 - 5s - loss: 0.0151 - accuracy: 0.9856\n",
      "649/649 - 6s - loss: 0.0150 - accuracy: 0.9809\n",
      "643/643 - 6s - loss: 0.0140 - accuracy: 0.9803\n",
      "525/525 - 5s - loss: 0.0128 - accuracy: 0.9837\n",
      "530/530 - 5s - loss: 0.0165 - accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "for fold in mod:\n",
    "    test_loss, test_acc = mod[fold].evaluate(test['X'][fold], test['y'][fold], verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "b = np.array([5,6,7,8])\n",
    "c = np.array([9,10,11,12])\n",
    "l = [a,b,c]\n",
    "np.mean(l, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc,vacc = [],[]\n",
    "for fold in history:\n",
    "    acc.append(history[fold].history['accuracy'])\n",
    "    vacc.append(history[fold].history['val_accuracy'])\n",
    "plt.plot(np.mean(acc, axis=0), label='accuracy', color='green')\n",
    "plt.plot(np.mean(vacc, axis=0), label = 'val_accuracy', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.9, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss,vloss = [],[]\n",
    "for fold in history:\n",
    "    loss.append(history[fold].history['loss'])\n",
    "    vloss.append(history[fold].history['val_loss'])\n",
    "    \n",
    "plt.plot(np.mean(loss, axis=0), label='Train', color='green')\n",
    "plt.plot(np.mean(vloss, axis=0), label='Validation', color='red')\n",
    "plt.xlabel('Epoch', size=16)\n",
    "plt.ylabel('Loss', size=16)\n",
    "plt.ylim([0., 0.1])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19064, 3) (19064,) (19064,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no fall       0.99      1.00      0.99     18681\n",
      "    pre-fall       0.63      0.28      0.39       233\n",
      "        fall       0.79      0.72      0.75       150\n",
      "\n",
      "    accuracy                           0.99     19064\n",
      "   macro avg       0.80      0.67      0.71     19064\n",
      "weighted avg       0.98      0.99      0.98     19064\n",
      "\n",
      "(20742, 3) (20742,) (20742,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no fall       0.99      0.99      0.99     20286\n",
      "    pre-fall       0.34      0.23      0.28       239\n",
      "        fall       0.83      0.65      0.73       217\n",
      "\n",
      "    accuracy                           0.98     20742\n",
      "   macro avg       0.72      0.62      0.66     20742\n",
      "weighted avg       0.98      0.98      0.98     20742\n",
      "\n",
      "(20555, 3) (20555,) (20555,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no fall       0.99      0.99      0.99     20169\n",
      "    pre-fall       0.34      0.48      0.40       202\n",
      "        fall       0.68      0.66      0.67       184\n",
      "\n",
      "    accuracy                           0.98     20555\n",
      "   macro avg       0.67      0.71      0.69     20555\n",
      "weighted avg       0.98      0.98      0.98     20555\n",
      "\n",
      "(16793, 3) (16793,) (16793,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no fall       0.99      0.99      0.99     16424\n",
      "    pre-fall       0.57      0.42      0.48       236\n",
      "        fall       0.78      0.72      0.75       133\n",
      "\n",
      "    accuracy                           0.98     16793\n",
      "   macro avg       0.78      0.71      0.74     16793\n",
      "weighted avg       0.98      0.98      0.98     16793\n",
      "\n",
      "(16935, 3) (16935,) (16935,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no fall       0.99      0.99      0.99     16589\n",
      "    pre-fall       0.30      0.11      0.17       174\n",
      "        fall       0.69      0.71      0.70       172\n",
      "\n",
      "    accuracy                           0.98     16935\n",
      "   macro avg       0.66      0.61      0.62     16935\n",
      "weighted avg       0.98      0.98      0.98     16935\n",
      "\n",
      "\n",
      "Overall\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no fall       0.99      0.99      0.99     92149\n",
      "    pre-fall       0.43      0.31      0.36      1084\n",
      "        fall       0.75      0.69      0.72       856\n",
      "\n",
      "    accuracy                           0.98     94089\n",
      "   macro avg       0.72      0.66      0.69     94089\n",
      "weighted avg       0.98      0.98      0.98     94089\n",
      "\n",
      "[[16497    41    51]\n",
      " [  150    20     4]\n",
      " [   45     5   122]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# X_train, y_train = train['X'][fold], train['y'][fold]\n",
    "# X_test,  y_test  =  test['X'][fold],  test['y'][fold]\n",
    "ypm, ytm = [], []\n",
    "for fold in mod:\n",
    "    y_pred = mod[fold].predict(test['X'][fold])\n",
    "    y_pred_max = np.argmax(y_pred, axis=1)\n",
    "    y_test_max = np.argmax(test['y'][fold], axis=1)\n",
    "    if len(ypm) == 0:\n",
    "        ypm = y_pred_max\n",
    "        ytm = y_test_max\n",
    "    else:\n",
    "        ypm = np.concatenate((ypm,y_pred_max))\n",
    "        ytm = np.concatenate((ytm,y_test_max))\n",
    "    print(y_pred.shape, y_pred_max.shape, y_test_max.shape)\n",
    "#    confusion_matrix(y_test_max, y_pred_max)\n",
    "    print(classification_report(y_test_max, y_pred_max, target_names=['no fall', 'pre-fall', 'fall']))\n",
    "print(\"\\nOverall\\n\")\n",
    "print(classification_report(ytm, ypm, target_names=['no fall', 'pre-fall', 'fall']))\n",
    "print(confusion_matrix(y_test_max, y_pred_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for classification\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Sensitivity} &= \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \\\\\n",
    "\\text{Specificity} &= \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} \\\\\n",
    "\\text{Accuracy} &= \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{TN} + \\text{FN}} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Label 0\n",
      "    accuracy\t0.983\n",
      " specificity\t0.488\n",
      " sensitivity\t0.993\n",
      " Label 1\n",
      "    accuracy\t0.987\n",
      " specificity\t0.995\n",
      " sensitivity\t0.311\n",
      " Label 2\n",
      "    accuracy\t0.995\n",
      " specificity\t0.998\n",
      " sensitivity\t0.686\n"
     ]
    }
   ],
   "source": [
    "def truth_test(_test,_pred, i):\n",
    "    _test = np.array(_test)\n",
    "    _pred = np.array(_pred)\n",
    "    \n",
    "    _pred_pos = _test[_pred == i]\n",
    "    _pred_neg = _test[_pred != i]\n",
    "    \n",
    "    _true_pos = len(_pred_pos[_pred_pos == i])\n",
    "    _fals_pos = len(_pred_pos[_pred_pos != i])\n",
    "    \n",
    "    _true_neg = len(_pred_neg[_pred_neg != i])\n",
    "    _fals_neg = len(_pred_neg[_pred_neg == i])\n",
    "    \n",
    "    return _true_pos, _fals_pos, _true_neg, _fals_neg\n",
    "\n",
    "def sensitivity(_test,_pred, i):\n",
    "    tp, fp, tn, fn = truth_test(_test, _pred, i)\n",
    "    return tp / ( tp + fn)\n",
    "\n",
    "def specificity(_test,_pred, i):\n",
    "    tp, fp, tn, fn = truth_test(_test, _pred, i)\n",
    "    return tn / ( tn + fp)\n",
    "\n",
    "def accuracy(_test, _pred, i):\n",
    "    tp, fp, tn, fn = truth_test(_test, _pred, i)\n",
    "    return (tp+tn) / (tp + fp + tn + fn)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\" Label\", i)\n",
    "    print(\"    accuracy\\t%5.3f\"%accuracy(ytm, ypm, i))\n",
    "    print(\" specificity\\t%5.3f\"%specificity(ytm, ypm, i))\n",
    "    print(\" sensitivity\\t%5.3f\"%sensitivity(ytm, ypm, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "import h5py\n",
    "# serialize model to JSON\n",
    "model_path = os.path.abspath(\"../../models/model_hybrid_20\")\n",
    "model_json = mod[3].to_json()\n",
    "with open(model_path+\".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "mod[3].save_weights(model_path+\".h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "model_path = os.path.abspath(\"../../models/\")\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow([[20047,    72,    50],\n",
    "            [  150,    48,     4],\n",
    "            [   47,     1,   136]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
